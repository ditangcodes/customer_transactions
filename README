# Customer Transaction and Churn Analysis
This project provides a set of functions for reading, processing, and analyzing customer transaction data from CSV files. The code includes tasks for loading data, converting files to Parquet format, aggregating transactions, identifying high-risk transactions, calculating cumulative balances, generating monthly transaction metrics, calculating average transactions per customer, and predicting customer churn using logistic regression.

This project provides functions using the ETL process for analysing customer transaction data from CSV files. The code shows how during the transformation stage, we changed the file format to parquets as well as:
- Aggregating transactions
- Identifying high-risk transactions
- Calculating cumulative balances
- Generating monthly transaction metrics
- Calculating average transactions per customer
- Predicting customer churn using logistic regression

## Table of Contents
1. [Dependencies](#dependencies)
2. [Function Descriptions](#function-descriptions)
   - [read_csv_file](#read_csv_file)
   - [comparing_columns](#comparing_columns)
   - [convert_file_into_parquet](#convert_file_into_parquet)
   - [aggregate_transactions_by_acc](#aggregate_transactions_by_acc)
   - [identify_high_risk_transactions](#identify_high_risk_transactions)
   - [cumulative_balances](#cumulative_balances)
   - [monthly_transactions](#monthly_transactions)
   - [average_transaction_per_customer](#average_transaction_per_customer)
   - [customer_churn](#customer_churn)
3. [Usage](#usage)

## Dependencies

The following Python libraries are required to run the code:
- `pandas`
- `matplotlib`
- `scikit-learn`
- `imblearn`
- `pyarrow`

## Function Descriptions
# 'read_csv_file'
Reads multiple CSV files and returns their content as a list of DataFrames. Strips white spaces from column names.

Parameters:

file_paths (list): List of paths to the CSV files.
Returns:

List of DataFrames.

# comparing_columns
Compares the number of rows and columns between two DataFrames.

Parameters:

original_df (DataFrame): Original DataFrame.
cleaned_df (DataFrame): Cleaned DataFrame.
Returns:

Boolean indicating whether the DataFrames have the same shape.
# convert_file_into_parquet
Converts a list of DataFrames to Parquet format and saves them to a specified directory.

Parameters:

data_frames (list): List of DataFrames.
file_location (str): Directory where Parquet files will be saved.
# aggregate_transactions_by_acc
# Optimizing Aggregation of Transaction Data
When working with large datasets to aggregate transaction information, consider the following best practices:

Efficient Memory Usage:
Loading extensive Parquet files into memory can strain resources. To optimize memory usage:
Dask: Explore using Dask, a tool that handles large datasets efficiently by performing computations out-of-core (without loading everything into memory at once).
If memory remains an issue, read the data in smaller chunks (e.g., using the chunksize parameter in pd.read_parquet).
Select Relevant Columns:
When merging different datasets (DataFrames), focus on essential columns:
For your use case, consider loading only the following columns: ‘customer_id’, ‘account_type’, ‘transaction_type’, and ‘amount’.
This reduces memory usage and speeds up processing.
Merge Smartly:
Instead of merging DataFrames multiple times, do it in a single step:
Use the merge function with all three DataFrames, specifying the join keys (e.g., ‘customer_id’).
This approach is more efficient and avoids unnecessary intermediate steps.
Parallelization for Speed:
If your dataset is substantial, consider parallelizing the aggregation process:
Tools like Dask or multiprocessing allow you to split the work across multiple cores or even machines.
Parallelization speeds up computation, especially for resource-intensive tasks.
Compress Your Data:
When saving the aggregated data to a file (e.g., Parquet), use compression:
Consider Snappy or Gzip compression algorithms.
Compression reduces the file size, making it faster to read and transfer.
Optimize Indexing:
If you frequently search or filter data based on specific columns (e.g., ‘customer_id’ or ‘account_type’), consider setting appropriate indexes:
Indexing significantly improves query performance.

Aggregated DataFrame.
# identify_high_risk_transactions
Identifies high-risk transactions (amount greater than a specified value) and saves the results to a Parquet file.

Parameters:

transactions_df (DataFrame): Transactions DataFrame.
amount (int, optional): Amount threshold to identify high-risk transactions. Defaults to 10000.
Returns:

DataFrame of high-risk transactions.
# cumulative_balances
Calculates cumulative balances for each account based on transactions and saves the results to a Parquet file.

Parameters:

accounts_file (str): Path to the accounts Parquet file.
transaction_file (str): Path to the transactions Parquet file.
Returns:

Dictionary of cumulative balances per account.
# monthly_transactions
Generates monthly transaction metrics (total amount, average amount, transaction count) and saves the results to a Parquet file.

Parameters:

transactions_file (str): Path to the transactions Parquet file.
Returns:

DataFrame of monthly transaction metrics.
# average_transaction_per_customer
Calculates the average transaction amount per customer and saves the results to a Parquet file.

Parameters:

transactions_file (str): Path to the transactions Parquet file.
customers_file (str): Path to the customers Parquet file.
Returns:

DataFrame of average transaction amount per customer.
# Churn Detection Model
Predicts customer churn based on transaction and customer data using logistic regression.


This script trains a churn detection model using customer and transaction data. The model predicts whether a customer is likely to churn based on their transaction history.

Step-by-Step Explanation:
Load Data:

We load the transactions and customers data from Parquet files into Pandas DataFrames.
Merge Data:

We combine the transactions and customers data into a single DataFrame based on common account IDs.
Calculate Last Transaction Date:

For each account, we find the date of the last transaction.
Prepare Data for Churn Calculation:

We merge the last transaction dates back into the main DataFrame.
We clean up any extra spaces and convert date columns to proper date formats.
Calculate Days Since Last Transaction and Join Date:

We calculate how many days have passed since the last transaction and since the customer joined.
Determine Churn:

For each threshold (30, 60, 90 days), we mark customers as churned if they haven’t made a transaction within that period.
We combine all these cases into a single DataFrame.
Check Data Balance:

We ensure the dataset has both churned and non-churned examples. If not, an error is raised.
Split Data into Training and Test Sets:

We split the data into training and test sets.
Train the Model:

We train a logistic regression model on the training set.
Evaluate the Model:

We make predictions on the test set and calculate the model’s accuracy.
Save the Model:

The trained model is saved for future use.
Example Data:
We use example data to simulate real scenarios:

Accounts Data:
Information about accounts, including their IDs, customer IDs, types, balances, and creation dates.
Customers Data:
Information about customers, including their IDs, names, emails, and join dates.
Transactions Data:
Information about transactions, including transaction IDs, account IDs, dates, amounts, and types.

# Reading CSV files
csv_files = ['../landing/accounts.csv', "../landing/customers.csv", "../landing/transactions.csv"]
loaded_data = read_csv_file(csv_files)

# Converting CSV files to Parquet
output_directory = "../staging"
convert_file_into_parquet(loaded_data, output_directory)

# Aggregating transactions
file1 = "../staging/accounts.csv_cleaned.parquet"
file2 = "../staging/transactions.csv_cleaned.parquet"
file3 = "../staging/customers.csv_cleaned.parquet"
aggregated_data = aggregate_transactions_by_acc(file1, file2, file3)

# Identifying high-risk transactions
transactions_df = pd.read_parquet(file2)
high_risk_df = identify_high_risk_transactions(transactions_df)

# Calculating cumulative balances
accounts_file = "../staging/accounts.csv_cleaned.parquet"
transactions_file = "../staging/transactions.csv_cleaned.parquet"
cumulative_balances_dict = cumulative_balances(accounts_file, transactions_file)

# Generating monthly transaction metrics
monthly_metrics_df = monthly_transactions(transactions_file)

# Calculating average transaction amount per customer
customers_file = "../staging/customers.csv_cleaned.parquet"
avg_transaction_per_customer = average_transaction_per_customer(transactions_file, customers_file)

# Predicting customer churn
trained_model = customer_churn(transactions_file, customers_file)